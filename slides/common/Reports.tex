%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluating Reports}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Outline}
    \begin{columns}[t]
        \begin{column}{.5\textwidth}
            \tableofcontents[sections={1-9},currentsection]
        \end{column}
        \begin{column}{.5\textwidth}
            \tableofcontents[sections={10-18},currentsection]
        \end{column}
    \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is this about?}
   \begin{question}[Questions]
   	 \begin{itemize}
        \item Is there a way to get benchmarking information?
        \item When I want to write a publication, which software(s) version(s) have I been using?
     \end{itemize}
   \end{question}
   \begin{docs}[Objectives]
   	  \begin{enumerate}
         \item Learn the implications and limitations of benchmarks.
         \item Learning how to generate and interpret reports with \Snakemake{}.
      \end{enumerate}
  \end{docs}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Benchmarking}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is ``Benchmarking'' in Computer Science?}
  \lhref{https://en.wikipedia.org/wiki/Benchmarking}{Benchmarking} may mean a number of things:
  \begin{itemize}[<+->]
   \item assessing software quality
   \item assessing software completeness (all the features we want?)
   \item assessing software usability
   \item \ldots
  \end{itemize}
  \pause
  \begin{docs}
  	What is most commonly meant: Getting performance and scalability markers.
  \end{docs}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Generating Benchmarks}
  Similar to crating a logfile, we are provided with a \altverb{benchmark}-directive, which can point to a benchmarking file:
  \begin{lstlisting}[language=Python,style=Python]
rule bwa_map:
    ...
    log:
        "logs/bwa_mem/{sample}.log"
    benchmark:
        "benchmarks/{sample}.bwa.benchmark.txt"
    ...
  \end{lstlisting} 
  \begin{docs}
  	\Snakemake{} will measure the wall clock time and memory usage (in MiB) and store it in the file in tab-delimited format.
  \end{docs}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Repeating Benchmarks}
  \vfill
  \begin{exampleblock}{Repeated benchmarks}
   It is possible to repeat a benchmark multiple times in order to get a sense for the variability of the measurements. This can be done by annotating the benchmark file, e.\,g., with \altverb{repeat("benchmarks/\{sample\}.bwa.benchmark.txt", 3)}. \Snakemake{} can be told to run the job three times. The repeated measurements occur as subsequent lines in the tab-delimited benchmark file.
  \end{exampleblock}
  \vfill
\end{frame} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{``Proper'' Benchmarking}
  A good estimate of a software's scalability takes into account:
  \begin{itemize}[<+->]
   \item running with different degrees of parallelization, e.\,g. 1-n threads
   \item not using toy inputs, but real data, possibly different real data
   \item avoiding I/O issues (common with alignment programs)
  \end{itemize}
  \pause
  \begin{warning}
  	We conclude: \Snakemake{}'s benchmarking capabilities are limited!
  \end{warning}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reporting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{\Snakemake{} Reports}
  Asking \Snakemake{} for a report on a completed workflow is as easy as:
  \begin{lstlisting}[language=Bash, style=Shell]
$ snakemake --report
  \end{lstlisting}
  This will generate a file called ``\altverb{report.html}'', which you can visualize with a browser.
\end{frame} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{\Snakemake{} Reports - adding Output}
  \begin{docs}
  	 Each output file that shall be part of the report has to be marked with the \altverb{report} flag, which optionally points to a caption in \lhref{https://docutils.sourceforge.io/docs/user/rst/quickstart.html}{restructured text format}.
  \end{docs}
  An example for our workflow would be:
  \begin{lstlisting}[language=Python,style=Python]
rule plot_quals:
    input:
        "calls/all.vcf"
    output:
        @report("plots/quals.svg",@ 
               @caption="report/qual.rst")@
    script:
        "scripts/plot-quals.py"
  \end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{\HandsOn{Writing an Annotation}}
  Let's write the file \altverb{report/qual.rst}! It shall contain our caption.
  \pause
  Our solution might(!) look like this:
  \begin{lstlisting}
Number of variations (deviations from reference) 
per experimental record. 
  \end{lstlisting}	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
	\frametitle{\HandsOn{Adding our Figure to the Report}}
	Now add the highlighted lines:
	\begin{lstlisting}[language=Python,style=Python]
rule plot_quals:
    input:
	    "calls/all.vcf"
	output:
		@report("plots/quals.svg",@ 
		@caption="report/qual.rst")@
	script:
		"scripts/plot-quals.py"
	\end{lstlisting}
    \begin{task}{Re-Run our report generator:}
    	\altverb{snakemake --report}
    \end{task}
\end{frame}


